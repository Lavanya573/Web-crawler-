# -*- coding: utf-8 -*-
"""google crawler for digital marketing  and sending email using google smtp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rbhJ9eb4DsQjG9gWl4G9u1-bu1alXGat

Imports & Global Variables
"""

import requests
import pandas as pd
from datetime import datetime

import json

# Global list to store questions
questions_list = []

"""Google Custom Search API Credentials"""

# Google Custom Search credentials
API_KEY = ""
CX = ""

# Google Search API URL
google_url = "https://www.googleapis.com/customsearch/v1"

"""Crawler Settings

"""

SITES = ["quora.com", "reddit.com"]
RESULTS_PER_SITE = 20

"""Google Search Function"""

def google_search(keyword, site):
    results = []

    for start in range(1, RESULTS_PER_SITE + 1, 10):
        params = {
            "key": API_KEY,
            "cx": CX,
            "q": f"{keyword} site:{site}",
            "start": start
        }

        r = requests.get(BASE_URL, params=params)

        if r.status_code != 200:
            break

        data = r.json()
        items = data.get("items", [])

        if not items:
            break

        for item in items:
            results.append({
                "Title": item.get("title"),
                "Link": item.get("link"),
                "Snippet": item.get("snippet"),
                "Source": site
            })

    return results

"""User Input"""

keyword = input("Enter keyword to search: ").strip()
safe_keyword = keyword.replace(" ", "_").lower()

"""Run Crawler"""

def google_search(keyword, site):
    results = []

    for start in range(1, RESULTS_PER_SITE + 1, 10):
        params = {
            "key": API_KEY,
            "cx": CX,
            "q": f"{keyword} site:{site}",
            "start": start
        }

        r = requests.get(google_url, params=params)

        if r.status_code != 200:
            break

        data = r.json()
        items = data.get("items", [])

        if not items:
            break

        for item in items:
            results.append({
                "Title": item.get("title"),
                "Link": item.get("link"),
                "Snippet": item.get("snippet"),
                "Source": site
            })

    return results

all_results = []

for site in SITES:
    all_results.extend(google_search(keyword, site))

df = pd.DataFrame(all_results)
df

filename = f"{safe_keyword}_crawler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
df.to_csv(filename, index=False, encoding="utf-8")

print(f"CSV saved as: {filename}")



"""Email using smtp

"""

import smtplib
import pandas as pd
from email.message import EmailMessage
import ssl
import os

"""Email Credentials"""

SMTP_EMAIL = "dumbi5558@gmail.com"
SMTP_APP_PASSWORD = "yfqc cdpm objw doig"

""" User Inputs

"""

csv_file = input("Enter CSV filename to email: ").strip()
receiver_email = input("Enter receiver email: ").strip()

"""Read CSV & Count Rows"""

df = pd.read_csv(csv_file)
total_rows = len(df)

print("Total rows in CSV:", total_rows)

"""Create Email Body"""

email = EmailMessage()
email["From"] = SMTP_EMAIL
email["To"] = receiver_email
email["Subject"] = "Crawler Results Ready"

email.set_content(
    f"""Web Crawler Completed

Total results collected: {total_rows}
Attached CSV file: {csv_file}
"""
)

"""Attach CSV File"""

with open(csv_file, "rb") as f:
    email.add_attachment(
        f.read(),
        maintype="application",
        subtype="octet-stream",
        filename=os.path.basename(csv_file)
    )

"""Send Email (Gmail SSL)"""

context = ssl.create_default_context()

with smtplib.SMTP_SSL("smtp.gmail.com", 465, context=context) as server:
    server.login(SMTP_EMAIL, SMTP_APP_PASSWORD)
    server.send_message(email)

print(" Email sent successfully")

